## Trends_Analysis_in_RealEstate

## Анализ трендов в строительстве с использованием NLP

### Dataset
Датасет состоит из сообщений собранных из 15 телеграмм-каналов:
- Движение.ру
- Всё о стройке
- Железобетонный замес
- Ипотека в России. Новости и аналитика
- Ипотека и недвижимость
- Недвижимость инсайды
- Торг уместен
- Прожектор новостроек – аналитика bnMAP.pro
- Не движется
- Треугольный метр
- РИА Недвижимость
- Пульс Продаж Новостроек
- Движ про недвиж
- Бойко о недвижимости
- Pro недвижимость. Smarent

Первоначально собрано 112 000 сообщений за 2019-2024 годы. В дальнейшем анализе будут использоваться данные только   2023 и 2024 годов.

## Notebook с кодом для парсинга телеграмм каналов.
https://colab.research.google.com/drive/1kkR_Z7Ilgl2pZeV8YaxHZBG0oSEWN5jO?usp=sharing

Код выполняет следующие действия для извлечения и обработки сообщений из Telegram-каналов:

- **Подключение к Telegram API:** Использует библиотеку Telethon для подключения к Telegram API с указанными `api_id`, `api_hash` и сессией.
- **Очистка текста сообщений:** Определяет функцию `remove_unwanted_characters` для удаления нежелательных символов из текста сообщений (оставляет только буквы, цифры, пробелы и основные знаки препинания).
- **Извлечение сообщений из канала:** Функция `fetch_messages_from_channel` получает историю сообщений из указанного канала Telegram.
    - Использует `GetHistoryRequest` для получения сообщений партиями (по 100 за раз).
    - Обрабатывает ошибки `FloodWaitError` (ограничение скорости запросов) и другие исключения.
    - Очищает текст каждого сообщения с помощью `remove_unwanted_characters`.
    - Сохраняет дату, текст и название канала в DataFrame.
- **Сбор данных из нескольких каналов:** Функция `main` выполняет следующие действия:
    - Инициализирует пустой DataFrame `combined_df`.
    - Итерируется по списку `channel_usernames` (предполагается, что этот список определен вне данного кода).
    - Для каждого канала вызывает `fetch_messages_from_channel` для получения DataFrame с сообщениями.
    - Объединяет DataFrame сообщений из каждого канала в `combined_df`.
    - Выводит `combined_df` в консоль.
- **Запуск асинхронного кода:** Использует `asyncio.run` для запуска асинхронной функции `main` и сохраняет результат (DataFrame) в переменную `df`.
- **Результат:** В итоге было скачано 112597 сообщений из Telegram каналов.

Данные сохранены и доступны в googlesheet:
https://docs.google.com/spreadsheets/d/1-3pzWa6GN4n0ukZEa2_e524j74kxpx1E/edit?usp=sharing&ouid=105790878003907527731&rtpof=true&sd=true


## Предварительный анализ текстов

### Предварительно произведем очистку и лемматизацию текстов

В сообщениях встречается набор имен собственных, в которых имеется точка. Этот могут быть наименования компаний или сайтов. Например: dvizhenie.ru, ДОМ.РФ, НН.ru
Для того, чтобы не потерять эти имена при очистке текстов от знаков препинания. Обработаем такие точки как исключения: заменив сначала их на _DOT_, а потом вернем точку обратно.

### Проанализируем длинну сообщений
![image](https://github.com/user-attachments/assets/944012af-47ae-498b-8d40-c0e66df5870d)

В дальнейшем будем убирать из датасета сообщения содержащие меньше 10 и больше 400 слов.

### Подсчитаем количество сообщений в каждом из кварталов 2023 и 2024 годов

Number of messages in each quarter:

2023Q1: 6236 messages
2023Q2: 6727 messages Прирост к предыдущему кварталу: 8%
2023Q3: 6787 messages Прирост к предыдущему кварталу: 1%
2023Q4: 6949 messages Прирост к предыдущему кварталу: 2%

2024Q1: 6527 messages Прирост к предыдущему кварталу: -6%
2024Q2: 7294 messages Прирост к предыдущему кварталу: 12%
2024Q3: 7331 messages Прирост к предыдущему кварталу: 1%
2024Q4: 8003 messages Прирост к предыдущему кварталу: 9%

Здесь можно сделать вывод, что значиельных и критических отличий в количестве сообщений нет. 
Но некоторая динамика на увеличение количеста сообщений имеется - в среднем рост до 10%, и падение количества сообщений только в 1 квартале 2024 года.

## Подход с попытной составления биграмм и триграмм различными способами для выявления наиболее часто встречающихся биграмм/триграмм
https://colab.research.google.com/drive/1iUsTnS8hMm6A4TD7c1ADYabfG2D8yyRj?usp=sharing

## Подход с попытной извлечения ключевых фраз состоящих из пяти слов
https://colab.research.google.com/drive/1jHeqE-EVpyDgwAbsVE463JwmETNIeP0p?usp=sharing

Здесь пробуем выделть ключевые слова с помощью функции extract_key_phrases, которая извлекает ключевые фразы (n-граммы) из входного текста, используя модель BERT. Вот что она делает по шагам:
1. Токенизация текста: Разбивает текст на субтокены с использованием tokenizer (предполагается, что он уже инициализирован).
2. Объединение субтокенов: Собирает субтокены в целые слова, обрабатывая специальные символы BERT, такие как ##.
3. Удаление служебных токенов: Отфильтровывает служебные токены, такие как [CLS], [SEP], и [PAD].
4. Создание n-грамм: Генерирует список n-грамм (последовательностей из n_grams слов).
5. Оценка важности n-грамм:
- Маскирует каждую n-грамму в исходном тексте (заменяет её на tokenizer.mask_token).
- Прогоняет замаскированный текст через модель BERT.
- Оценивает уверенность модели в предсказании замаскированного слова (или слов, если n-грамма состоит из нескольких токенов). Более высокая уверенность предполагает, что n-грамма важна для понимания смысла текста.
6. Выбор top-k фраз: Выбирает top_k n-грамм с наивысшими оценками уверенности BERT.
7. Возврат ключевых фраз: Возвращает список выбранных ключевых фраз.

Здесь пример выделенных ключевых слов. Рассматриваем результат при трех перезапусках расчета. У нас получилось три варианта, то есть при каждом перезапуске получаем отличные наборы слов:
- key_phrases
- key_phrases_2
- key_phrases_3
![image](https://github.com/user-attachments/assets/f2521cdb-cd8c-493f-ac71-3a27d26fd478)


# Далее попробуем кластеризовать сообщения и саммаризовать кластера с использованием LLM "gpt-3.5-turbo"

## Подход с разбиением на 20 кластеров
https://colab.research.google.com/drive/13c2h2kGJ17Mk60ZEa8txBflQ-ZMWveYk?usp=sharing

Описание пайплайна по шагам:
0. Загрузка данных.
1. TF-IDF Vectorization:
Использует TfidfVectorizer для преобразования текстовых сообщений в числовые векторы. Это делается с использованием подхода TF-IDF (Term Frequency-Inverse Document Frequency), который отражает важность слов в каждом сообщении относительно всего корпуса текстов.
Настраивает векторизатор:
Исключает русские стоп-слова (наиболее часто встречающиеся слова, не несущие особого смысла, например, предлоги и союзы) и английские стоп-слова.
max_df=0.95 означает игнорировать слова, которые встречаются в более чем 95% документов (слишком общие).
min_df=2 означает игнорировать слова, которые встречаются менее чем в 2 документах (слишком редкие).
Результатом является TF-IDF матрица, где строки соответствуют сообщениям, а столбцы - словам.
2. Выбор количества компонент для SVD:
Определяет количество компонент для Truncated SVD. Использует минимум из 50 и (количество столбцов в TF-IDF матрице - 1). Это делается для ограничения количества компонент, чтобы избежать переобучения и снизить вычислительную сложность.
3. Применение Truncated SVD:
   Использует Truncated SVD (Singular Value Decomposition) для уменьшения размерности TF-IDF матрицы. SVD выделяет наиболее важные темы (компоненты) в данных, что помогает улучшить результаты кластеризации.
n_components определяет количество компонент, которые будут сохранены.
Выводится объясненная доля дисперсии, чтобы оценить, насколько хорошо SVD сохранила информацию из исходных данных.
5. Кластеризация с использованием K-Means:
Использует алгоритм K-Means для кластеризации сообщений на основе их SVD-представлений.
n_clusters определяет количество кластеров (в данном случае 20).
n_init=10 задает количество запусков K-Means с разными центрами кластеров, чтобы выбрать лучшее решение.
Результаты кластеризации (номера кластеров) добавляются в DataFrame в столбец cluster.
6. Анализ кластеров по кварталам:
Добавляет столбец quarter в DataFrame, содержащий номер квартала, к которому относится сообщение (на основе столбца date).
Вычисляет общее количество сообщений в 3-м и 4-м кварталах.
Создает DataFrame cluster_summary_df, который содержит:
Количество сообщений в каждом кластере в 3-м и 4-м кварталах.
Процент сообщений в каждом кластере в 3-м и 4-м кварталах относительно общего количества сообщений в соответствующем квартале.
Изменение количества и процентного соотношения сообщений в каждом кластере между 3-м и 4-м кварталами.
7. LLM-Based Topic Summarization (Суммирование темы кластера с использованием LLM):
Использует большую языковую модель (LLM), в частности GPT-3.5 Turbo от OpenAI, для автоматического определения и описания тем каждого кластера.
Функция summarize_cluster_topic принимает список сообщений из кластера и генерирует краткое описание темы на русском языке.
Использует API OpenAI для взаимодействия с моделью.
Для каждого кластера выбирается случайная выборка из 10 сообщений, которые используются в качестве входных данных для LLM.
Сгенерированные описания тем сохраняются в словаре cluster_topic_summaries.
8. Добавление LLM-резюме в DataFrame:
Добавляет столбец topic_summary в cluster_summary_df, содержащий описание темы каждого кластера, сгенерированное LLM.
9. Отображение сводной таблицы кластеров с темами LLM:
Выводит сводную таблицу cluster_summary_df с информацией о кластерах, включая количество и процентное соотношение сообщений в каждом квартале, изменение этих показателей и описание темы кластера, сгенерированное LLM.
Использует стилизацию DataFrame для более наглядного представления данных (форматирование процентов, цветовая подсветка изменений).
10. Генерация облака слов и отображение сообщений (опционально):
Для каждого кластера:
Генерирует облако слов (Word Cloud) на основе лемматизированных сообщений. Это визуальный способ представления наиболее часто встречающихся слов в кластере.
Отображает несколько случайных сообщений из кластера для более детального анализа.

В итоге получился такой список 20 кластеров для анализа. В столбце persentage_change выведено процентное изменение в доле кластера.

![image](https://github.com/user-attachments/assets/84a45295-d0a6-4519-9da1-69ea6bdb5208)
![image](https://github.com/user-attachments/assets/868d465f-236a-4720-a436-8012c8612ff4)
![image](https://github.com/user-attachments/assets/de797d16-ab1d-475a-b462-0e4a50a84e70)

По описанию составленному LLM многие кластера выглядят похожими, кажется как будто в некоторых кластерах нет четко выделенной темы, либо она сформулирована невнятно, либо в кластере оказалось настолько много разнообразных сообщений, что невозможно выделить общую тему.
Потребуется дальнейшая работа, которая возможна в следующих направлениях:
- Gодбор оптимального количества кластеров
- Использование других алгоритмов кластеризации. Например, DBScan
- Использование другого алгоритма векторизации
- Работа с промптом для LLM.
