## Trends_Analysis_in_RealEstate - Анализ трендов в строительстве с использованием NLP

### Dataset
Датасет состоит из сообщений собранных из 15 телеграмм-каналов:
- Движение.ру - 13 000 подписчиков
- Всё о стройке - 66 000 подписчиков
- Железобетонный замес - 64 000 подписчиков
- Ипотека в России. Новости и аналитика - 29 000 подписчиков
- Ипотека и недвижимость - 61 000 подписчиков
- Недвижимость инсайды - 101 000 подписчиков
- Торг уместен - 24 000 подписчиков
- Прожектор новостроек – аналитика bnMAP.pro - 5 000 подписчиков
- Не движется - 66 000 подписчиков
- Треугольный метр - 32 000 подписчиков
- РИА Недвижимость - 37 000 подписчиков
- Пульс Продаж Новостроек - 11 000 подписчиков
- Движ про недвиж - 71 000 подписчиков
- Бойко о недвижимости - 11 000 подписчиков
- Pro недвижимость. Smarent - 39 000 подписчиков
Количество подписчиков указано по состоянию  на январь 2025 года.

Первоначально собрано 112 000 сообщений за 2019-2024 годы. В дальнейшем анализе будут использоваться данные только 2023 и 2024 годы.

## Notebook с кодом для парсинга телеграмм каналов.
https://colab.research.google.com/drive/1kkR_Z7Ilgl2pZeV8YaxHZBG0oSEWN5jO?usp=sharing

Код выполняет следующие действия для извлечения и обработки сообщений из Telegram-каналов:

- **Подключение к Telegram API:** Использует библиотеку Telethon для подключения к Telegram API с указанными `api_id`, `api_hash` и сессией.
- **Очистка текста сообщений:** Определяет функцию `remove_unwanted_characters` для удаления нежелательных символов из текста сообщений (оставляет только буквы, цифры, пробелы и основные знаки препинания).
- **Извлечение сообщений из канала:** Функция `fetch_messages_from_channel` получает историю сообщений из указанного канала Telegram.
    - Использует `GetHistoryRequest` для получения сообщений партиями (по 100 за раз).
    - Обрабатывает ошибки `FloodWaitError` (ограничение скорости запросов) и другие исключения.
    - Очищает текст каждого сообщения с помощью `remove_unwanted_characters`.
    - Сохраняет дату, текст и название канала в DataFrame.
- **Сбор данных из нескольких каналов:** Функция `main` выполняет следующие действия:
    - Инициализирует пустой DataFrame `combined_df`.
    - Итерируется по списку `channel_usernames` (предполагается, что этот список определен вне данного кода).
    - Для каждого канала вызывает `fetch_messages_from_channel` для получения DataFrame с сообщениями.
    - Объединяет DataFrame сообщений из каждого канала в `combined_df`.
    - Выводит `combined_df` в консоль.
- **Запуск асинхронного кода:** Использует `asyncio.run` для запуска асинхронной функции `main` и сохраняет результат (DataFrame) в переменную `df`.
- **Результат:** В итоге было скачано 112597 сообщений из Telegram каналов.

Данные сохранены и доступны в googlesheet:
https://docs.google.com/spreadsheets/d/1-3pzWa6GN4n0ukZEa2_e524j74kxpx1E/edit?usp=sharing&ouid=105790878003907527731&rtpof=true&sd=true


## Предварительный анализ текстов

### Предварительно произведем очистку и лемматизацию текстов

В сообщениях встречается набор имен собственных, в которых имеется точка. Этот могут быть наименования компаний или сайтов. Например: dvizhenie.ru, ДОМ.РФ, НН.ru
Для того, чтобы не потерять эти имена при очистке текстов от знаков препинания. Обработаем такие точки как исключения: заменив сначала их на _DOT_, а потом вернем точку обратно.
Данные сохранены в таблице:
https://docs.google.com/spreadsheets/d/1-1O62ZmNs5Ayo-zJNHUPDAH8fscvo_9V/edit?usp=sharing&ouid=105790878003907527731&rtpof=true&sd=true

### Проанализируем длинну сообщений
![image](https://github.com/user-attachments/assets/944012af-47ae-498b-8d40-c0e66df5870d)

В дальнейшем будем убирать из датасета сообщения содержащие меньше 10 и больше 400 слов.

### Подсчитаем количество сообщений в каждом из кварталов 2023 и 2024 годов

Number of messages in each quarter:


2023Q1: 6236 messages

2023Q2: 6727 messages Прирост к предыдущему кварталу: 8%

2023Q3: 6787 messages Прирост к предыдущему кварталу: 1%

2023Q4: 6949 messages Прирост к предыдущему кварталу: 2%


2024Q1: 6527 messages Прирост к предыдущему кварталу: -6%

2024Q2: 7294 messages Прирост к предыдущему кварталу: 12%

2024Q3: 7331 messages Прирост к предыдущему кварталу: 1%

2024Q4: 8003 messages Прирост к предыдущему кварталу: 9%

Здесь можно сделать вывод, что значиельных и критических отличий в количестве сообщений нет. 
Но некоторая динамика на увеличение количеста сообщений имеется - в среднем рост до 10%, и падение количества сообщений только в 1 квартале 2024 года.

## Подход с попыткой составления биграмм и триграмм различными способами для выявления наиболее часто встречающихся биграмм/триграмм
https://colab.research.google.com/drive/1iUsTnS8hMm6A4TD7c1ADYabfG2D8yyRj?usp=sharing

## Подход с попыткой извлечения ключевых фраз состоящих из пяти слов
https://colab.research.google.com/drive/1jHeqE-EVpyDgwAbsVE463JwmETNIeP0p?usp=sharing

Здесь пробуем выделть ключевые слова с помощью функции extract_key_phrases, которая извлекает ключевые фразы (n-граммы) из входного текста, используя модель BERT. Вот что она делает по шагам:
1. Токенизация текста: Разбивает текст на субтокены с использованием tokenizer (предполагается, что он уже инициализирован).
2. Объединение субтокенов: Собирает субтокены в целые слова, обрабатывая специальные символы BERT, такие как ##.
3. Удаление служебных токенов: Отфильтровывает служебные токены, такие как [CLS], [SEP], и [PAD].
4. Создание n-грамм: Генерирует список n-грамм (последовательностей из n_grams слов).
5. Оценка важности n-грамм:
- Маскирует каждую n-грамму в исходном тексте (заменяет её на tokenizer.mask_token).
- Прогоняет замаскированный текст через модель BERT.
- Оценивает уверенность модели в предсказании замаскированного слова (или слов, если n-грамма состоит из нескольких токенов). Более высокая уверенность предполагает, что n-грамма важна для понимания смысла текста.
6. Выбор top-k фраз: Выбирает top_k n-грамм с наивысшими оценками уверенности BERT.
7. Возврат ключевых фраз: Возвращает список выбранных ключевых фраз.

Здесь пример выделенных ключевых слов. Рассматриваем результат при трех перезапусках расчета. У нас получилось три варианта, то есть при каждом перезапуске получаем отличные наборы слов:
- key_phrases
- key_phrases_2
- key_phrases_3
![image](https://github.com/user-attachments/assets/f2521cdb-cd8c-493f-ac71-3a27d26fd478)

## Дополнительная подготовка датасета
Код содержится в ноутбуке: https://drive.google.com/file/d/1OSdDF26ywso9snsPK01UfSj919zWJB8u/view?usp=sharing
- Уберем из датасета сообщения, в которых менее 20 и более 400 слов
- Добавим в датасет информацию о длинне сообщения
- Добавим поле msg_id с номером строки, это пригодится в дальнейшей работе

Сохраняем датасет в файле df_lemmatized_no_short_msg.xlsx и дальше работаем с этим файлом

# Далее попробуем кластеризовать сообщения и саммаризовать кластера с использованием LLM "gpt-3.5-turbo"

## Подход с разбиением на 20 кластеров
https://colab.research.google.com/drive/13c2h2kGJ17Mk60ZEa8txBflQ-ZMWveYk?usp=sharing

Описание пайплайна по шагам:
0. **Загрузка данных.**
1. **TF-IDF Vectorization:**
Использует TfidfVectorizer для преобразования текстовых сообщений в числовые векторы. Это делается с использованием подхода TF-IDF (Term Frequency-Inverse Document Frequency), который отражает важность слов в каждом сообщении относительно всего корпуса текстов.
**Настраивает векторизатор:**
Исключает русские стоп-слова (наиболее часто встречающиеся слова, не несущие особого смысла, например, предлоги и союзы) и английские стоп-слова.
max_df=0.95 означает игнорировать слова, которые встречаются в более чем 95% документов (слишком общие).
min_df=2 означает игнорировать слова, которые встречаются менее чем в 2 документах (слишком редкие).
Результатом является TF-IDF матрица, где строки соответствуют сообщениям, а столбцы - словам.
2. **Выбор количества компонент для SVD:**
Определяет количество компонент для Truncated SVD. Использует минимум из 50 и (количество столбцов в TF-IDF матрице - 1). Это делается для ограничения количества компонент, чтобы избежать переобучения и снизить вычислительную сложность.
3. **Применение Truncated SVD:**
   Использует Truncated SVD (Singular Value Decomposition) для уменьшения размерности TF-IDF матрицы. SVD выделяет наиболее важные темы (компоненты) в данных, что помогает улучшить результаты кластеризации.
n_components определяет количество компонент, которые будут сохранены.
Выводится объясненная доля дисперсии, чтобы оценить, насколько хорошо SVD сохранила информацию из исходных данных.
5. **Кластеризация с использованием K-Means:**
Использует алгоритм K-Means для кластеризации сообщений на основе их SVD-представлений.
n_clusters определяет количество кластеров (в данном случае 20).
n_init=10 задает количество запусков K-Means с разными центрами кластеров, чтобы выбрать лучшее решение.
Результаты кластеризации (номера кластеров) добавляются в DataFrame в столбец cluster.
6. **Анализ кластеров по кварталам:**
Добавляет столбец quarter в DataFrame, содержащий номер квартала, к которому относится сообщение (на основе столбца date).
Вычисляет общее количество сообщений в 3-м и 4-м кварталах.
Создает DataFrame cluster_summary_df, который содержит:
Количество сообщений в каждом кластере в 3-м и 4-м кварталах.
Процент сообщений в каждом кластере в 3-м и 4-м кварталах относительно общего количества сообщений в соответствующем квартале.
Изменение количества и процентного соотношения сообщений в каждом кластере между 3-м и 4-м кварталами.
7. **LLM-Based Topic Summarization (Суммирование темы кластера с использованием LLM):**
Использует большую языковую модель (LLM), в частности GPT-3.5 Turbo от OpenAI, для автоматического определения и описания тем каждого кластера.
Функция summarize_cluster_topic принимает список сообщений из кластера и генерирует краткое описание темы на русском языке.
Использует API OpenAI для взаимодействия с моделью.
Для каждого кластера выбирается случайная выборка из 10 сообщений, которые используются в качестве входных данных для LLM.
Сгенерированные описания тем сохраняются в словаре cluster_topic_summaries.
8. **Добавление LLM-резюме в DataFrame:**
Добавляет столбец topic_summary в cluster_summary_df, содержащий описание темы каждого кластера, сгенерированное LLM.
9. Отображение сводной таблицы кластеров с темами LLM:
Выводит сводную таблицу cluster_summary_df с информацией о кластерах, включая количество и процентное соотношение сообщений в каждом квартале, изменение этих показателей и описание темы кластера, сгенерированное LLM.
Использует стилизацию DataFrame для более наглядного представления данных (форматирование процентов, цветовая подсветка изменений).
10. **Генерация облака слов и отображение сообщений:**
Для каждого кластера:
Генерирует облако слов (Word Cloud) на основе лемматизированных сообщений. Это визуальный способ представления наиболее часто встречающихся слов в кластере.
Отображает несколько случайных сообщений из кластера для более детального анализа.

В итоге получился такой список 20 кластеров для анализа. В столбце persentage_change выведено процентное изменение в доле кластера.

![image](https://github.com/user-attachments/assets/84a45295-d0a6-4519-9da1-69ea6bdb5208)
![image](https://github.com/user-attachments/assets/868d465f-236a-4720-a436-8012c8612ff4)
![image](https://github.com/user-attachments/assets/de797d16-ab1d-475a-b462-0e4a50a84e70)

По описанию составленному LLM многие кластера выглядят похожими, кажется как будто в некоторых кластерах нет четко выделенной темы, либо она сформулирована невнятно, либо в кластере оказалось настолько много разнообразных сообщений, что невозможно выделить общую тему.
Потребуется дальнейшая работа, которая возможна в следующих направлениях:
- Подбор оптимального количества кластеров
- Использование других алгоритмов кластеризации. Например, вместо K-means использовать DBScan
- Использование другого алгоритма векторизации
- Работа с промптом для LLM.

## Расчитаем заново сравнение 3 и 4 квартала, но не для 20, а для 50 кластеров
https://colab.research.google.com/drive/1XECYIrPcWxpN7F5gHEzV88p8t7pbajBS?usp=sharing

Используем пока что те же методы, что и в предыдущем подходе.

Выведем таблицу с 5 кластерами наиболее увеличившимися в проценте доли от общего количества
![image](https://github.com/user-attachments/assets/d2892854-4033-4d18-980b-8efa689b98b4)


## Расчитаем сравнение поочередно всех кварталов для 50 кластеров.
- Также здесь добавлена визуализациия кластеров, после сокращения размерности CVD матриц до 2 главных компонент.
![image](https://github.com/user-attachments/assets/ba476de9-e50d-4497-97a3-246a98d7f012)
![image](https://github.com/user-attachments/assets/85f46496-2230-4e9c-bd0f-4b3b9cde7ad4)


# Дальнейший анализ с использованием более продвинутых алгоритмов
ссылка на ноутбук:
https://drive.google.com/file/d/1w8DU4SicPtSikVTgQ7OgiZFv9s_aw3ty/view?usp=sharing

Расчет эмбеддингов с моделью Bert

Кластеризация с помошью алгоритма DBSCAN
![image](https://github.com/user-attachments/assets/9430c7e7-e6df-4ec7-a74f-99c99cc2d96e)

Получилось всего 140 кластеров. Самый большой это -1 кластер - выбросы.
![image](https://github.com/user-attachments/assets/2657b5a2-1288-4f93-8de9-17f0608835cd)

Побъединяем кластера и смотрим TF-IDF слова внутри кластера:
![image](https://github.com/user-attachments/assets/15c3d723-1faf-478a-ba56-21064b0a25e9)

Топ 15 кластеров
![image](https://github.com/user-attachments/assets/2dfd3ddd-6c74-410b-96e9-8d19861e0cea)

Сравнение тем по кварталам:
![image](https://github.com/user-attachments/assets/f16378cb-09c0-4976-b75e-bb34106a1612)
![image](https://github.com/user-attachments/assets/bca5f239-bc05-4255-8056-8ccca860fbf7)
![image](https://github.com/user-attachments/assets/2537e4d3-154b-4543-b253-ebc77a806ab2)
![image](https://github.com/user-attachments/assets/319f6063-1476-47bc-96cb-1914ecb8ca71)
![image](https://github.com/user-attachments/assets/293d961e-d470-4dcc-9f79-4da7314cff7c)
![image](https://github.com/user-attachments/assets/83e26825-c081-45b8-92c2-2df43e7912ab)

Более наглядно можно представить изменение размера кластеров в динамике по кварталам и по месяцам:

![image](https://github.com/user-attachments/assets/89a64d8d-f857-4d4a-b57d-2ae42fa838cc)
![image](https://github.com/user-attachments/assets/5e0e9549-54b9-497e-8044-bd51217f0c72)






